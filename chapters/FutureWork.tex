\chapter{Future Work}

Our implementation, like many other LZ factorizaton implementations, was just a proof of concept to show compression speeds and compression ratio.
Although we do output the correct pairs needed, we could take it further and encode them in a way that decompressers can understand.
In doing so, we could create an actual utility to be used to compress actual data.

One aspect that was not considered in this thesis was the effect of having previous knowledge of the input.
Specifically, what can we do if we know the alphabet of the input is limited.
For instance during the suffix array construction, we do an initial sort of the 2/3 group using a 3 character prefix.
To do this, we need to use three radix sorts.
If we know exactly how many bits represent the largest character or integer in the alphabet, we can specialize the radix sort to only sort on those bits.
If this is not possible, we could also check if three characters could fit into a smaller number of characters and perform a lesser number of radix sorts on them.

Recent work has looked at different ways to solve the ANSV problem.
Work done in \cite{Computing longest previous factor in linear time and applications} and \cite{Simpler and Faster Lempel Ziv Factorization} has explored a technique called peak elimination to solve the ANSV problem.
It is unclear whether this solution would parallelize and fit on the GPU architecture.
They also found success using a single data strucute to hold both the PSVs and NSVs to improve memory locality.

Another very important measurement that we did not consider was the space efficiency of our algorithm.
As inputs, such as DNA sequences, grow larger and larger, it is important to make sure the algorithm is as space efficient as possible, so that the algorithm can scale.
Recent work by \cite{Space EfÔ¨Åcient Linear Time Lempel-Ziv Factorization on Constant Size Alphabets} has shown methods to reduce the space needed by LZ factorization algorithms by reusing the space required by auxillary data structures.
It is especially important when working on the GPU, where hardware limits are stricter, memory is more sparse, and the communication overhead to go back and forth from the GPU to the CPU is expensive.

Multiple GPU support is becoming increasingly popular as GPU applications become more mainstream.
Enabling multiple GPU support would allow our implementation to handle larger inputs.
It would be interesting to investigate the added communication overhead and its effects on the overall performance.
Multiple GPU support would also allow us to accompany high performance users, who have machines or clusters of machines with single or multiple GPUs.

A simpler optimization that could be added in future implementations is a more dynamic and adaptable kernel launch parameters.
In our implementation, we left many of the kernel paramters as program launch parameters for exhaustive trial and error.
Other kernel parameters were also optimized specifically to the GPU we used for evaluation.
Our implementation would still work with other GPUs, but different parameters might find faster compression speeds.
One approach is to gather information about the GPU using the CUDA API before launching any kernels.
We can then use that information to generate more sensible grid and block sizes.
We can also use templating features for greater flexibility.
Many CUDA libraries make use of this approach to great success.

NVIDIA CUDA is still a growing framework, as new hardware and new API releases add additional features to ease or enable programmers.
Recent releases have enabled dynamic parallelism and a unified memory.
Dynamic parallelism allows GPU kernels to launch additional kernels from the kernels themselves.
Traditionally, the GPU is used as a coprocessor and kernels must be launched from instructions on the CPU.
Using dynamic parallelism, added overhead from communication between the GPU and CPU can me circumvented.
Dynamic parallelism can also allow for easier or more load balanced parallelism.
For example, during the final LZ factorization calculation, we could launch a new CUDA kernel to do string comparisons instead of having threads in a block working together.
The benefits of unified memory in our current implementation is not clear.
Since most of the data is generated and remains on the GPU, unified memory may only simplify the communication while not adding performance benefits.
It would still be interesting to evaluate if these new features could provide speedups.

As of now, our implementation works only on NVIDIA GPUs through the use of CUDA.
Although GPGPU development is dominated by NVIDIA CUDA on NVIDIA GPUS, the graphics market share includes many other significant vendors, including AMD and Intel.
There are a variety of methods to create an implementation for use with those other vendors.
The first is to rewrite the implementation using OpenCL.
OpenACC, GPU Ocelot
Furthermore, the usage of BLZ could be examined on different platforms, where the cost to perform string comparisons are not as expensive, like a multicore CPU.

